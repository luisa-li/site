<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>luisa's blog</title>
        <link rel="stylesheet" href="/public/base.css"/>
        <link rel="stylesheet" href="/public/blog/blogpost.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
        <link rel="icon" type="image/png" href="/public/resources/tau.png">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>    
    </head>
    <body>
        <div class="center-box">
        <nav class="left-navbar">
            <a href="/blog">
                <i class="fas fa-arrow-left icon"></i> Back
            </a>
        </nav>
        <div class="head">
            <div class="title-data">
                <h2> 
                    <span class="yellow-highlight">to</span>
                    <span class="blue-highlight">invertibility</span>
                    <span class="red-highlight">and</span>
                    <span class="aqua-highlight">beyond</span>
                </h2>
                <p> may, 2025 </p>
            </div>
            <p id="intro"> i put thing in black box, then i want thing back, but how? </p>
        </div>
        <div class="body">

            <p class="whisper"> this blog aims to give a structured introduction on how we can make neural networks invertible. note that this blog assumes a good level of knowledge of probability theory, calculus, linear algebera, and machine learning principles, though i do try and define some terms along the way. of course, suggestions are always welcome! </p>

            <p class="whisper"> this ended up being incredibly long and technical, you have <a href="https://jakegines.in/"><span class="blue-highlight"><u>jake</u></span></a> to thank for that. </p>

            <h3 class="section-title"> a primer on invertibility </h3>

            <p> in caveman terms, an invertible process is one we know how to undo. moving a banana is invertible, eating the banana is not. </p>

            <p> mathematically, an invertible function is a bijective one, and a bijective function is both injective and surjective. these are defined as follows: </p>
            <ul>
                <li> injective: \( F(x) = F(y) \implies x=y \) </li>
                <li> surjective: \( \forall \, b \in B,  \exists \, a \in A\) such that \(F(a) = b \) </li>
            </ul>
            <p> an injective function is such that one output is given by one input exactly, with no two inputs giving the same output, and a surjective function is such that every possible output is mapped to by some input. together, they give a bijection. given any output, i know the input that mapped to it exists and is unique. </p>

            <p> example o' clock, the function \(y = 4x + 10\) is invertible, because given any \(y\), i can give you back \(x\) via the inverse function \(x = \frac{y - 10}{4}\). note that this function is analytically invertible, because we are able to give a closed form expression of the inverse function that gives us back the input exactly. </p>

            <p> without going into the much detail, it is pretty easy to see that most neural networks are <b> not </b> analytically invertible in their usual forms. for one, the most common choice of activation function, \( ReLU(x) = max(0, x) \), would render the whole thing non-invertible. if we receive a positive value as output to ReLU, we can be sure that that is the same number we put in. but we are certainly out of luck if a negative number is spit out. </p>

            <p> since the composition of functions is only invertible iff each function in the composition is invertible, the usage of ReLU as an activation function to a linear makes the whole thing non-invertible. beyond ReLU and linear layers, common operations like convolutions on images and message passing on graphs are also not analytically invertible because of their aggregation steps. simply put, the result of an addition tells you nothing about the things you added to get there. </p>

            <p class="question"> so is all hope lost? can we go home now? </p>

            <p class="response"> no, not yet. </p>

            <p class="aside"> <span class="blue-highlight"> the hard and soft ways to invertibility </span> </p>

            <p> before we move on to discussions about neural networks, it is useful to make a distinction between hard and soft invertibility. this will make our mission precise, as well as yield a general framework for us to pursue. </p>

            <p> loosely speaking, soft invertibility is encouraged, and hard invertibility is enforced. encouraging a model to be invertible involves regularizing it via a loss term that punishes poor invertibility. enforcing a model to be invertible, on the other hands, means architectural designs that guarantees the model to be invertible, regardless of how the training run goes. </p>

            <p> a classic example of a soft invertible model is the autoencoder, which is trained to learn an efficient encoding of unlabeled data. it consists of two networks, an encoder \( E_{\phi} \) and a decoder \( D_{\theta} \), the former encodes the input data into a latent space, and the latter decodes it back into the original data space. the autoencoder is trained to minimize reconstruction loss, which measures the distance between \(x\), the original input, and \( x' = D_{\theta}(E_{\phi}(x)) \), the result of pushing \( x \) through the network and back. </p>

            <p> super important to note is that soft invertibility may only be accurately invertible on the data distribution it is trained on. in other words, an autoencoder that recovers pictures of sharks accurately might not generalize to pictures of chairs. on the other hand, hard invertibility gives us a guarantee that any input will be invertible. in this blog, we are in pursuit of hard invertibility, and so our next stop is naturally to look at constraining our model architecture to enforce this property. </p>

            <h3 class="section-title"> normalizing flows: flowing towards the normal </h3>

            <p class="whisper"> a comprehensive overview of normalizing flows can be found in <a href="https://arxiv.org/abs/1908.09257">this review</a></p>

            <p> normalizing flows are one class of generative models that bakes hard invertibility into its architecture. unlike a discriminator whose objective is to estimate a label conditioned on its input variales: \( p (y|x_1, x_2, ..., x_n) \), a generative model aims to learn the joint probability distribution \( p(x_1, x_2, ..., x_n) \) over all its input variables. generative models are attractive because of their flexibility - given a well trained model, we can turn it into a discriminator using bayes' rule, use it to perform density estimation, and generate new samples from the original data distribution. </p>

            <p class="whisper"> see section 1.1 of kingma et al's <a href="https://arxiv.org/abs/1906.02691">introduction to variational autoencoders</a> for a great explanation of distriminative v.s generative modeling and motivation for the latter. </p>

            <p> the key idea to normalizing flow is the following: all of our data is drawn from some unknown data distribution, but this distribution may be arbitrarily complex and therefore hard to sample new data points from. however, if we are able to transform this input data distribution into a simpler distribution via a series of invertible and differentiable mappings, then we would be able to generate new samples simply by sampling from the simple distribution (say that 10 times fast), and applying the inverse of the mappings one by one. most commonly, that simpler distribution is the standard normal distribution, hence the name of the model - we are flowing our data distribution towards the normal. </p>

            <p> generative models aim to capture the underlying data generation process, and so one of the central objectives during training is to maximize the likelihood of seeing the observed data under our model. in other words, we want to optimize the parameters so that it assigns high probability to the data points we do observe. this process is known as maximum likelihood estimation, and is the primary learning objective for models that aim to explicitly model the entire joint distribution. our current subject of study, the normalizing flow family of models, is an example of explicit generative models. </p>

            <p class="whisper"> for some great intuition on maximum likelihood and bayesian statistics, see <a href="https://allendowney.github.io/ThinkBayes2/index.html">think bayes 2</a> </p>

            <p> maximum likelihood estimation requires us to know how to compute the likelihood. lucky us, the transformations used by normalizing flows are invertible and differentiable, which enables exact computation of the likelihood via the change of variables formula. </p>
            
            <p> the change of variables formula from probability theory describes exactly how we would go about computing likelihoods resulting from these invertible and differentiable mappings: </p>

            <div class="container">
              <div class="box">
                <p class="box-title">Change of Variables Formula</p>
                <p>
                  Given \( Z \in \mathbb{R}^D \), a random variable with a known and tractable probability density function \( p_Z: \mathbb{R}^D \to \mathbb{R} \), if \( g \) is an invertible function and \( Y = g(Z) \), let \( f = g^{-1} \). Then the probability density function of the random variable \( Y \) is:
                </p>
                <p>
                  \[ p_Y(y) = p_Z(f(y)) \left| \det \left( \frac{\partial f}{\partial y} \right) \right| \]
                </p>
                <p>
                  Here, \( \det \left( \frac{\partial f}{\partial y} \right) \) denotes the determinant of the Jacobian matrix of \( f \), evaluated at \( y \).
                </p>
              </div>
            </div>
            
            <p>this formula is ✨ <span class="yellow-highlight">illuminating</span> ✨ because it prescribes the necessary conditions to build an invertible model that allows for exact and tractable likelihood computations throughout training: we need a bijectiion that is expressive when stacked in layers, and we need this bijection, its inverse and the determinant of its jacobian to be speedily computable. if we had such a function, we would have collected all the puzzle pieces to a invertible and explicitly generative model. </p>

            <p class="response"> and of course, we <b>do</b> have such a function. </p>

            <h3 class="section-title"> a couple of coupling layers </h3>

            <p class="whisper"> original paper <a href="https://arxiv.org/abs/1410.8516">here</a></p>

            <p> coupling layers as proposed by dihn et al gives us just that. the core idea behind this sort of layer is to split the input \( x \) to each layer into two pieces, \(x_1\) and \(x_2\) and perform the following computation, with \( m \) being an arbitrarily complex function. </p>

            <p> \begin{align*} y_1 &= x_1 \\ y_2 &= x_2 + m(x_1) \end{align*} </p>

            <p> a little bit of rearranging very trivially gives us the inverse function: </p>

            <p> \begin{align*} x_1 &= y_1 \\ x_2 &= y_2 - m(y_1) \end{align*} </p>

            <p> note that both the forward and inverse computation of the coupling layer can happen with just the forward pass of the function \(m\), which can be as simple or as complex as the builder of the model desires it to be. </p>

            <p> of course, the coupling layer leaves part of the input completely unchanged, but this issue of expressibility can be fixed simply by having multiple coupling layers, and alternating the block that receives the identity function at each turn. composition of invertible functions is invertible, and so this stacking of coupling layers gives us the analytically invertible that we sought for. </p>

            <p> all we need now is a fast way to compute the determinant of the jacobian... </p>

            <p class="response"> do you remember your matrix determinant rules? </p>

            <p class="question"> O_O </p>

            <div class="box">
              <div class="container">

                <p class="box-title"> aside: bitesized determinants </p>
                <p> the determinant of a matrix \(A\) is nonzero iff \(A\) is invertible, and it is intuitively the factor by which space is stretched after applying \(A\). </p>

                <p class="whisper"> there is a beautiful <a href="https://www.youtube.com/watch?v=Ip3X9LOh2dk">3b1b video</a> with great visualizations on this stretching action! </p>

                <p> how to compute it? well. you can sit down with your pencil and get it via laplace expansion or LU decomposition or a variety of other methods, giving you a time complexity of \( O(n^3) \). or, if your matrix is well behaved (not a mathematical definition), you can apply a number of properties of the determinant to compute it <b> way </b> faster. </p>

                <p> one such useful property, is that the determinant of a triangular matrix is simply the product of its diagonal: </p>

                <p> \[ \text{If } A \text{ is triangular, then } \det(A) = \prod_{i=1}^{n} A_{ii} \]</p>
                <p> proof is left as an excercise for the readers (: </p>

              </div>
            </div>

            <p> in the case of coupling layers, the determinant of the jacobian is simply 1. here's all the formulas again, explicitly written out: </p>

            <p>
                \[
                x = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}, \quad
                y = \begin{bmatrix} y_1 \\ y_2 \end{bmatrix}, \quad
                y_1 = x_1, \quad y_2 = x_2 + m(x_1).
                \]
            </p>

            <p> \[ J = \frac{\partial x}{\partial y} = 
                \begin{bmatrix}
                \frac{\partial x_1}{\partial y_1} & \frac{\partial x_1}{\partial y_2} \\
                \frac{\partial x_2}{\partial y_1} & \frac{\partial x_2}{\partial y_2}
                \end{bmatrix} \] 
            </p>
            
            <p> since \(y_1 = x_1\), \( \frac{\partial x_1}{\partial y_1} = I \),  similarly, \( \frac{\partial x_2}{\partial y_2} = I \) </p>
            <p> and since \( y_2 \) does not depend on \(x_2\) at all, \( \frac{\partial x_1}{\partial y_2} = 0 \)</p> 

            <p> putting those blocks together, we get a lower triangular matrix, with ones along the diagonal: </p>

            <p> \[
            J = \begin{bmatrix}
            1 & 0 \\
            \frac{\partial x_2}{\partial y_1} & 1
            \end{bmatrix}
            \] </p>

            <p> by our aside on determinants, the determinant of the jacobian of this particular coupling layer is just 1, and we are able to drop this term entirely from our change of variables formula. this significantly simplifies our likelihood computations for this type of layers, and finally gives us the analytically invertible, explicitly generative model that we have worked so hard for. </p>

            <p> many other types of coupling layers can be constructed using similar logic - desinging \( \det \left( \frac{\partial f}{\partial y} \right) \) to be easily computable. dihn et al cover a few more variations, and prove that they also satisfy the conditions for invertibility and computational efficiency. similarly, coupling layers are not the only way to build normalizing flows, but again, similar principles apply. </p>

            <p class="question"> but wait, you keep saying analytic invertibility like there is somehow a secret, second form of invertibility... can invertibility exist without a explicit inverse? </p>

            <p class="response"> *nods mysteriously, with a smug smirk* </p>

            <h3 class="section-title"> 1-lipschitz neural networks </h3>

            <p class="whisper"> <a href="https://arxiv.org/abs/1802.05957">miyato et al.</a> spectral normalization for gans, and <a href="https://arxiv.org/abs/1811.00995">behrmann et al.</a> </p>

            <p> central to behrmann et al.'s thesis is that there is a very nice similarity between <a href="https://arxiv.org/abs/1512.03385">residual layers</a> and <a href="https://en.wikipedia.org/wiki/Euler_method">euler's method</a> for numerically solving ordinary differential equation (ODE) initial value problems (IVP): </p>

            <p> \[ x_{t+1} \leftarrow x_t + g_{\theta_t}(x_t) \quad \text{ResNet} \] </p>
            <p> \[ x_{t+1} \leftarrow x_t + h f_{\theta_t}(x_t) \quad \text{ODE IVP} \] </p>

            <p> for clarity, here's a summary on the notation for each: </p>

            <div class="table-container">
              <table border="1" cellpadding="8" cellspacing="0">
                  <thead>
                    <tr>
                      <th>component</th>
                      <th>residual layer</th>
                      <th>euler's method</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>update rule</td>
                      <td>\( x_{t+1} = x_t + g_{\theta_t}(x_t) \)</td>
                      <td>\( x_{t+1} = x_t + f_{\theta_t}(x_t) \)</td>
                    </tr>
                    <tr>
                      <td>\( x_t \)</td>
                      <td>input at layer \( t \)</td>
                      <td>state at time \( t \)</td>
                    </tr>
                    <tr>
                      <td>\( g_{\theta_t} \), \( f_{\theta_t} \)</td>
                      <td>learned residual function</td>
                      <td>derivative function</td>
                    </tr>
                  </tbody>
              </table>
            </div>

            <p> given this similarity, the behrmann et al. argued that the we can think about a residual neural network as a ODE problem and interpret the inverse of this ODE's IVP as the dynamics of the system going backwards in time. this backwards step looks as follows: </p>

            <p> \[ x_{t} \leftarrow x_{t+1} - g_{\theta_t}(x_t) \quad \text{ResNet} \] </p>
            <p> \[ x_{t} \leftarrow x_{t+1} - h f_{\theta_t}(x_t) \quad \text{ODE IVP} \] </p>

            <p> solving this backwards dynamic would implement an inverse of our corresponding resnet. in other words, if we are able to use \( g_{\theta_t} \) and \( x_{t+1 }\) to get \( x_t \), then we would be able to invert a residual layer as defined above. </p>

            <p> concretely, this is done by finding the fixed point of the function \(T\) defined as follows: </p>

            <p> \[ T(x) = x_{t + 1} - g_{\theta_t} (x) \] </p>

            <p class="whisper"> what is a fixed point, you ask? well... </p>

            <div class="container">
              <div class="box">
                <p class="box-title">Fixed Point</p>
                <p> 
                  Formally, \( c \) is a fixed point of a function \( f \) if \( c \) belongs to both the domain and the codomain of \( f \), and \(f(c) = c\).
                </p>
                <p>
                  Less formally, a fixed point is a value that does not change under a given transformation. For functions, it is a element that is mapped to itself under the function.
                </p>
              </div>
            </div> 

            <p> if we are able to find the fixed point to \( T \), we would have solved the backwards dynamics. since this fixed point \(x\) is exactly the inverse: </p>

            <p> \[ T(x) = x \quad \text{by definition of a fixed point} \] </p>
            
            <p> \[ T(x) = x_{t + 1} - g_{\theta_t} (x) \] </p>

            <p> \[ x = x_{t + 1} - g_{\theta_t} (x) \implies x_{t + 1} = x + g_{\theta_t}(x) \] </p>

            <p> the last equation shows that \( x \) was the input to the residual layer whose output was \( x_{t + 1} \), and so the fixed point \( x = x_t \) is the solution to the backwards dynamics. </p>
            
            <p> as is with a lot of mathematics, we've solved one problem, and our reward is two more: </p>

            <ul>
              <li> how can we guarantee that this fixed point exist and is unique? </li>
              <li> how do we concrete compute this fixed point? </li>
            </ul>

            <p> thankfully, banach's fixed point theorem addresses both of our problems in one fell swoop. </p>

            <p class="aside"> <span class="blue-highlight"> banach's fixed point theorem (and lipschitz continuity) </span> </p>

            <p class="whisper"> wikipedia pages for <a href="https://en.wikipedia.org/wiki/Banach_fixed-point_theorem">banach's fixed point theorem</a>, <a href="https://en.wikipedia.org/wiki/Lipschitz_continuity">lipschitz continuity</a> and <a href="https://en.wikipedia.org/wiki/Contraction_mapping">contraction mapping</a></p>

            <p> to start, obligatory math definitions, in very serious capitalizations. </p>

            <div class="container">
                <div class="box">
                  <p class="box-title"> Banach's Fixed Point Theorem </p>
                  <p> Let \( (X, d) \) be a non-empty complete metric space with a contraction mapping \( T: X \to X \). Then \( T \) admits a unique fixed point \( x^* \in X \) (i.e., \( T(x^*) = x^* \)). Furthermore, \( x^* \) can be found as follows: start with an arbitrary element \( x_0 \in X \) and define a sequence \( (x_n)_{n \in \mathbb{N}} \) by \( x_n = T(x_{n-1}) \, \text{for } n \geq 1 \). Then: \( \lim_{n \to \infty} x_n = x^* \). </p>
                </div>
            </div>

            <p> banach's fixed point theorem essentially guarantees a unique fixed point on a mapping, if the mapping is a contraction. it also provides a numerical way to iterate to converge to this fixed point. this method is aptly known as <span class="blue-highlight">fixed point iteration</span>.</p>

            <p class="whisper"> now. what is a contraction mapping? and how does it relate to lipschitz continuous functions? </p>

            <div class="container">
              <div class="box">
                <p class="box-title"> Lipschitz Continuity </p>

                  <p> Given two metric spaces \((X, d_X)\) and \((Y, d_Y)\), where \(d_X\) denotes the metric on the set \(X\) and \(d_Y\) is the metric on set \(Y\), a function \(f : X \rightarrow Y\) is called Lipschitz continuous if there exists a real constant \(K \geq 0\) such that, for all \(x_1\) and \(x_2\) in \(X\), </p>
                  
                  \[ d_Y(f(x_1), f(x_2)) \leq K \cdot d_X(x_1, x_2) \]
                  
                  <p> Any such \(K\) is referred to as a Lipschitz constant for the function \(f\). </p>               
              </div>
            </div>
            
            <div class="container">
              <div class="box">
                <p class="box-title"> Contraction Mapping </p>
                <p> A contraction mapping on a metric space \((M, d)\) is a function \(f\) from \(M\) to itself, with the property that there is some real number \(0 \leq k < 1\) such that for all \(x\) and \(y\) in \(M\), </p>
                <p> \[ d(f(x), f(y)) \leq k \cdot d(x, y). \] </p>
              </div>
            </div>

            <p> a contraction mapping is thus a special case of a lipschitz continuos function. specifically, it is one with a lipschitz constant of less than one, also known as a 1-lipschitz function. </p>
            
            <p> intuitively, it is helpful to think of the lipschitz constant as an upper bound on how much a function can change between any two points, which gives a even stronger guarantee on continuity than an upper bound on derivatives and thus the instantaneous change. </p>

            <p class="whisper"> the wikipedia article on <a href="https://en.wikipedia.org/wiki/Lipschitz_continuity">lipschitz continuity</a> has a very good animation illustrating this intuition </p>
            
            <p> ok. we want to guarantee a unique fixed point and a way to compute it, and banach's fixed point theorem gives us that. in exchange, it wants a contraction mapping, which requires a lipschitz constant of less than one. looking back to our original backwards dynamic... </p>

            <p> \[ x_{t} \leftarrow x_{t+1} - h f_{\theta_t}(x_t) \] </p>         

            <p>... it should be clear that we just need our \( f_{\theta_t}\) to have be 1-lipschitz. </p>

            <p> fortunately, 1-lipschitz layers have already been used as a mechanism to stabilize training for the discriminator in a generative adversarial network, see <a href="https://arxiv.org/abs/1802.05957">miyato et al.</a>. to put it simply, one can achieve such a 1-lipschitz layer by normalizing the weight matrix by its spectral norm, which is also known as its largest singular value: </p>

            \[ \bar{W} := \frac{W}{\sigma(W)} \]

            <p> and that, finally gives us an invertible neural network layer with unconstrained form. what it also gives us, is a blueprint for extending invertibility beyond fully connected and convolution layers. given a layer, we just need to prove that can be made 1-lipschitz. of course, much care is needed in practice to ensure the expressibility and usability of such layers, but we won't worry about that. </p>
            
            <h4 class="aside"> ...and scene </h4>

            <p class="postscript"> <span class="highlight"> postscript: </span> ways to achieve hard invertibility in neural networks is something that i'm three months deep in the rabbit hole for. starting off with autoencoders, which are invertible by gentle encouragement and only on its input data distribution. to finding normalizing flows as a niche and cool idea to achieve analytical invertibility, but being unhappy with its architectural constraints. to accidentally stumbling upon a seemingly soft invertible network that was just a little too invertible on out of distribution data, and digging through references and wikipedia pages. to finally finding the mathematical framework that back the claims of invertibility. it was quite the journey, and i hope that this blog emulates at least part of that. </p>

            <p class="postscript"> <span class="highlight"> postscript 2: </span> shoutout to <a href="https://ebrmn.space/"><span class="red-highlight"><u>eddie</u></span></a>, my trusty companion on this long and treacherous road to invertibility and beyond. i am beyond grateful for the countless hours of back and forth yaps (: </p>

        </div>
    </body>
</html>
