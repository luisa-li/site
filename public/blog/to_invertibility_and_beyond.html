<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>luisa's blog</title>
        <link rel="stylesheet" href="/public/base.css"/>
        <link rel="stylesheet" href="/public/blog/blogpost.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
        <link rel="icon" type="image/png" href="/public/resources/tau.png">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>    
    </head>
    <body>
        <div class="center-box">
        <nav class="left-navbar">
            <a href="/blog">
                <i class="fas fa-arrow-left icon"></i> Back
            </a>
        </nav>
        <div class="head">
            <div class="title-data">
                <h2> 
                    <span class="yellow-highlight">to</span>
                    <span class="blue-highlight">invertibility</span>
                    <span class="red-highlight">and</span>
                    <span class="aqua-highlight">beyond</span>
                </h2>
                <p> may, 2025 </p>
            </div>
            <p id="intro"> i put thing in black box, then i want thing back, but how? </p>
        </div>
        <div class="body">

            <p class="whisper"> this blog aims to give a structured introduction on how we can make neural networks invertible. of course, this is not a comprehensive or rigorous review, hence the pointers for further reading. enjoy! :3  </p>

            <h3 class="section-title"> a primer on invertibility </h3>

            <p> in caveman terms, an invertible process is one we know how to undo. moving a banana is invertible, eating the banana is not. </p>

            <p> mathematically, an invertible function is a bijective one, and a bijective function is both injective and surjective. these are defined as follows: </p>
            <ul>
                <li> injective: \( F(x) = F(y) \implies x=y \) </li>
                <li> surjective: \( \forall \, b \in B,  \exists \, a \in A\) such that \(F(a) = b \) </li>
            </ul>
            <p> in human words, an injective function is such that one output is given by one input exactly, with no two inputs giving the same output, and a surjective function is such that every possible output is mapped to by some input. together, they give a bijection. given any output, i know the input that mapped to it exists and is unique. </p>

            <p> example o' clock, the function \(y = 4x + 10\) is invertible, because given any \(y\), i can give you back \(x\) via the inverse function \(x = \frac{y - 10}{4}\). note that this function is analytically invertible, because we can find an analytical form of the inverse function. </p>

            <p> without going into the math, it is pretty easy to see that most neural networks are <b> not </b> analytically invertible. for one, the most common choice of activation function, ReLU, would render the whole thing non-invertible: </p>

            <p> \[ ReLU(x) = max(0, x) \]</p>

            <p> intuitively, if i got a positive number out of ReLU, i would know that that positive number is exactly the number that i put in. but if the output of ReLU is 0, then we would have no idea what we put in. and, since a composition of functions is only invertible if all pieces of the composition are invertible, the usage of ReLU renders the most common linear layer \( y = \phi(w^Tx + b)\) not analytically invertible. beyond linear layers, things like convolutions on images and message passing on graphs are also not invertible under similar reasoning. in both cases, the aggregation step renders the whole thing not analytically invertible - simply put, the result of an addition tell you nothing about what each component might have been. </p>

            <p class="question"> so is all hope lost? can we go home now? </p>

            <p class="response"> no, not yet. </p>

            <h3 class="section-title"> normalizing flows & flowing towards normality </h3>

            <p> never give up. </p>

            <p> normalizing flows are a class of generative models. think of stable diffusion and the gpts: its objective is to produce more samples in the distribution of data it is trained on. if i feed it pictures ikea sharks, it should learn how to generate more ikea shark looking plushies. </p>

            <p> the key idea of normalizing flows is as follows: if we are able to transforming the data distribution into a simpler distribution via a series of invertible and differentiable mappings, like the standard normal (hence the name), we would be able to generate new samples in the data distribution by simply sampling noise from the standard normal, and pushing the noise back through the inverse functions. </p>

            <p> this class of generative models is at its core based on the change of variables formula in probability theory, which gives us a way to compute the resulting (or the <i> pushforward</i>) distribution after applying a function to a distribution. </p>

            <p> given \( Z \in \mathbb{R}^D\), a random variable with a known and tractable probability density function \( p_z: \mathbb{R}^D \rightarrow \mathbb{R} \), if \( g \) is an invertible function, and \( Y = g(z) \), then the change of variables formula gives us a way to compute the probability density function of the random variable \( Y\). </p>

            <p> \[ p_Y(y) = p_Z(f(y)) \left| \det \frac{\partial f}{\partial y} \right| \] </p>
            
            <p> the inverse of this transformation can be similarly computed as long as \(g\) is invertible. this formula is informative in that it prescribes the necessary condition to building such an invertible layer. </p>
                
            <p class="whisper"> for a more comprehensive overview and a much more mathematically rigorous definition of normalizing flows see <a href="https://arxiv.org/abs/1908.09257">this review</a>.</p>

            <p class="whisper"> this change of variable formula is also useful for inverse transform sampling, which is not super relevant but super cool, you can read more about that <a href="https://www.bishopbook.com/"> in the bishop & bishop deep learning book, chapter 14.</a>.</p>

            <p> essentially. we want a bijection \( g \) that is expressive when stacked in layers. and importantly for building and training a model, we need this bijection, its inverse and the determinant of its jacobian to be easily computable. if we had such a function, we would be able to build such a normalizing flow to our hearts contents. </p>

            <p class="question"> that is, <b> if</b> we had such a function. </p>

            <p class="response"> and we do. </p>

            <h3 class="section-title"> coupling layers and their acquaintances </h3>

            <p class="whisper"> original paper <a href="https://arxiv.org/abs/1410.8516">here</a></p>

            <p> coupling layers as proposed by dihn et al gives us such a invertible function. the core idea behind this sort of layer is to split the input \( x \) to each layer into two pieces, \(x_1\) and \(x_2\) and perform the following computation, with \( m \) being an arbitrarily complex function. </p>

            <p>  \begin{align*} y_1 &= x_1 \\ y_2 &= x_2 + m(x_1) \end{align*} </p>

            <p> a little bit of rearranging very trivially gives us the inverse function: </p>

            <p> \begin{align*} x_1 &= y_1 \\ x_2 &= y_2 - m(y_1) \end{align*} </p>

            <p> note that both the forward and inverse computation of the coupling layer can happen with just the forward pass of the function \(m\), which can be as simple or as complex as the builder of the model desires it to be. </p>

            <p> of course, the coupling layer leaves part of the input completely unchanged, but this issue of expressibility can be fixed simply by having multiple coupling layers, and alternating the block that receives the identity function at each turn. composition of invertible functions is invertible, and so this stacking of coupling layers gives us the analytically invertible that we sought for. </p>

            <p> and that, my chums, is two out of three of our requirements to build a usable normalizing flow. all we need now is a fast way to compute the determinant of the jacobian of the function. </p>

            <p class="response"> do you remember how to compute a matrix determinant? </p>

            <p class="question"> o_o </p>

            <div class="box">
              <div class="container">

                <p class="box-title"> aside: bitesized determinants </p>
                <p> the determinant of a matrix \(A\) is nonzero iff \(A\) is invertible, and it intuitively the factor by which space is stretched after applying \(A\). </p>

                <p class="whisper"> there is a beautiful <a href="https://www.youtube.com/watch?v=Ip3X9LOh2dk">3b1b video</a> with great visualizations on this topic! </p>

                <p> how to compute it? well. you can dog it via laplace expansion or LU decomposition or a variety of other methods, giving you a time complexity of \( O(n^3) \). or, if your matrix is well behaved (not a mathematical definition), you can apply a number of properties of the determinant to compute it way faster. </p>

                <p> one such useful property, is that the determinant of a triangular matrix is simply the product of its diagonal: </p>

                <p> \[ \text{If } A \text{ is triangular, then } \det(A) = \prod_{i=1}^{n} A_{ii} \]</p>
                
                <p> a proof can (probably) be found in <a href="https://math.mit.edu/~gs/linearalgebra/ila6/indexila6.html">gil strang's linear algebra textbook</a> </p>

              </div>
            </div>

            <p> in our case, the determinant of the jacobian is simply 1. here's why: </p>

            <p>
                \[
                x = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}, \quad
                y = \begin{bmatrix} y_1 \\ y_2 \end{bmatrix}, \quad
                y_1 = x_1, \quad y_2 = x_2 + m(x_1).
                \]
            </p>

            <p> \[ J = \frac{\partial x}{\partial y} = 
                \begin{bmatrix}
                \frac{\partial x_1}{\partial y_1} & \frac{\partial x_1}{\partial y_2} \\
                \frac{\partial x_2}{\partial y_1} & \frac{\partial x_2}{\partial y_2}
                \end{bmatrix} \] 
            </p>
            
            <p> since \(y_1 = x_1\), \( \frac{\partial x_1}{\partial y_1} = I \),  similarly, \( \frac{\partial x_2}{\partial y_2} = I \) </p>
            <p> and since \( y_2 \) does not depend on \(x_2\) at all, \( \frac{\partial x_1}{\partial y_2} = 0 \)</p> 

            <p> putting those blocks together, we get a lower triangular matrix, with ones along the diagonal: </p>

            <p> \[
            J = \begin{bmatrix}
            1 & 0 \\
            \frac{\partial x_2}{\partial y_1} & 1
            \end{bmatrix}
            \] </p>

            <p> by our aside on determinants, the determinant of the jacobian of this particular coupling layer is just 1, and we are able to drop this term entirely from our change of variables formula. </p>

            <p> many such coupling layers can be constructed using similar logic in designing the \( \det(J) \) to be easily computable, the authors of the linked paper cover a few more variations, and prove that they also satisfy the necessary conditions for invertibility and computational efficiency. </p>

            <p> similarly, coupling layers are not the only way to build normalizing flows. but similar principles and goals apply when making such designs. again, i refer yall to <a href="https://arxiv.org/abs/1908.09257">this review</a> for further reading. </p>

            <p class="question"> but wait, you keep saying analytic invertibility like there is somehow a secret, second form of invertibility... can invertibility exist without a explicit inverse? </p>

            <p class="response"> *nods mysteriously, with a smug smirk* </p>

            <h3 class="section-title"> 1-lipschitz neural networks </h3> 

            <p class="whisper"> <a href="https://arxiv.org/abs/1811.00995">link to paper</a> </p>

            <p> there is a nice similarity between resisual layers and euler's method for numerically solving ordinary differential equation (ODE) initial value problems (IVP): </p>

            <p> \[ x_{t+1} \leftarrow x_t + g_{\theta_t}(x_t) \] </p>
            <p> \[ x_{t+1} \leftarrow x_t + h f_{\theta_t}(x_t) \] </p>

            <p> where the top equation is the standard <a href="https://arxiv.org/abs/1512.03385">residual layer</a> update rule, and the bottom equation is <a href="https://en.wikipedia.org/wiki/Euler_method">euler's method</a> with \( h \) being the step size, for clarify, here is a summary of the notation. </p>

            <div class="table-container">
              <table border="1" cellpadding="8" cellspacing="0">
                  <thead>
                    <tr>
                      <th>component</th>
                      <th>residual layer</th>
                      <th>euler's method</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>update rule</td>
                      <td>\( x_{t+1} = x_t + g_{\theta_t}(x_t) \)</td>
                      <td>\( x_{t+1} = x_t + f_{\theta_t}(x_t) \)</td>
                    </tr>
                    <tr>
                      <td>\( x_t \)</td>
                      <td>input at layer \( t \)</td>
                      <td>state at time \( t \)</td>
                    </tr>
                    <tr>
                      <td>\( g_{\theta_t} \), \( f_{\theta_t} \)</td>
                      <td>learned residual function</td>
                      <td>derivative function</td>
                    </tr>
                  </tbody>
              </table>
            </div>
              
            <p> given this similarity, we can think about the inverse of this ODE IVP as the dynamics of the system going backwards in time. more importantly, we can write this as an iteration that converges to the <span class="blue-highlight"> fixed point</span>, which is the inverse for a given function: </p>

            <p> \[ x_{t} \leftarrow x_{t+1} - g_{\theta_t}(x_t) \] </p>
            <p> \[ x_{t} \leftarrow x_{t+1} - h f_{\theta_t}(x_t) \] </p>         
            
            <p class="question"> so what does this **actually** mean for us? </p>

            <p> well, it means that if we can guarantee that this fixed point (which is also our inverse) exists and is unique. and a way to compute this fixed point will give us the inverse of the residual layer that we yearn for. </p>

            <p class="response"> and the two missing ingredients our answer are... </p>

            <p class="aside"> <span class="blue-highlight"> banach's fixed point theorem, and lipschitz continuity </span> </p>

            <p class="whisper"> wikipedia pages for <a href="https://en.wikipedia.org/wiki/Banach_fixed-point_theorem">banach's fixed point theorem</a>, <a href="https://en.wikipedia.org/wiki/Lipschitz_continuity">lipschitz continuity</a> and <a href="https://en.wikipedia.org/wiki/Contraction_mapping">contraction mapping</a></p>

            <p> to start, obligatory math definitions, in very serious capitalizations. </p>

            <div class="container">
                <div class="box">
                  <p class="box-title"> Banach's Fixed Point Theorem </p>
                  <p> Let \( (X, d) \) be a non-empty complete metric space with a contraction mapping \( T: X \to X \). Then \( T \) admits a unique fixed point \( x^* \in X \) (i.e., \( T(x^*) = x^* \)). Furthermore, \( x^* \) can be found as follows: start with an arbitrary element \( x_0 \in X \) and define a sequence \( (x_n)_{n \in \mathbb{N}} \) by \( x_n = T(x_{n-1}) \, \text{for } n \geq 1 \). Then: \( \lim_{n \to \infty} x_n = x^* \). </p>
                </div>
            </div>

            <p> banach's fixed point theorem essentially guarantees a unique fixed point on a mapping, if the mapping is a contraction. it also provides a numerical way to iterate to converge to this fixed point. this method is aptly known as <span class="blue-highlight">fixed point iteration</span>.</p>

            <p class="whisper"> now. what is a contraction mapping? and how does it relate to lipschitz continuous functions? </p>

            <div class="container">
              <div class="box">
                <p class="box-title"> Lipschitz Continuity </p>

                  <p> Given two metric spaces \((X, d_X)\) and \((Y, d_Y)\), where \(d_X\) denotes the metric on the set \(X\) and \(d_Y\) is the metric on set \(Y\), a function \(f : X \rightarrow Y\) is called Lipschitz continuous if there exists a real constant \(K \geq 0\) such that, for all \(x_1\) and \(x_2\) in \(X\), </p>
                  
                  \[ d_Y(f(x_1), f(x_2)) \leq K \cdot d_X(x_1, x_2) \]
                  
                  <p> Any such \(K\) is referred to as a Lipschitz constant for the function \(f\). </p>               
              </div>
            </div>
            
            <div class="container">
              <div class="box">
                <p class="box-title"> Contraction Mapping </p>
                <p> A contraction mapping on a metric space \((M, d)\) is a function \(f\) from \(M\) to itself, with the property that there is some real number \(0 \leq k < 1\) such that for all \(x\) and \(y\) in \(M\), </p>
                <p> \[ d(f(x), f(y)) \leq k \cdot d(x, y). \] </p>
              </div>
            </div>

            <p> a contraction mapping is thus a  </p>

        </div>
    </body>
</html>
